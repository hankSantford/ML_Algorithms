# 机器学习算法笔记
## 内容提要：
* 学习方法概述
  * 分类学习
  * 集成学习
  * 回归问题
* 机器学习算法
  * 随机森林（RF）
   * ID3决策树
   * C4.5决策树
   * CART决策树
  * 提升决策树（BSTDT）
  * 投票决策树（BAGDT）
  * 决策树比较（Bagging,Boosting,Randomization）
  * 支持向量机(SVM)
  * 朴素贝叶斯（NB）
### 学习方法概述

#### 集成学习
  集成方法是机器学习中的一类算法，其基本思想是构建多个不同的预测模型；此时的预测各个模型之间可以是同质，如：都采用决策树个体分类器，或都采用神经网络个体分类器，此类学习器个体之间有强关联采用算法以`Boosting系列`算法为代表，此类学习器个体之间有弱关联以‘Bagging和RandomForests系列算法’为代表；也可以是异质的，如：个体学习器可以采用支持向量机，逻辑回归，朴素贝叶斯等。然后将其输出做某种组合作为最终的输出，例如：可以通过对不同预测模型取平均值或采用投票方式来完成对于模型预测结果的集成。
  <br></br>
#### 分类学习
#### 惩罚性回归
惩罚性回归方法是由最小二乘法（OLS）衍生出来的。惩罚性回归设计之初的想法是为了克服最小二惩罚的根本缺陷——过拟合。

### 机器学习算法原理
#### 随机森林（RandomForest）
随机森林由LeoBreiman（2001）提出，顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。具体而言，它通过自助法（bootstrap）重采样技术，从原始训练样本集N中有放回地重复随机抽取k个样本生成新的训练样本集合，然后根据自助样本集生成k个分类树组成随机森林，新数据的分类结果按分类树投票多少形成的分数而定。其实质是对决策树算法的一种改进，将多个决策树合并在一起，每棵树的建立依赖于一个独立抽取的样品，森林中的每棵树具有相同的分布，分类误差取决于每一棵树的分类能力和它们之间的相关性。特征选择采用随机的方法去分裂每一个节点，然后比较不同情况下产生的误差。能够检测到的内在估计误差、分类能力和相关性决定选择特征的数目。单棵树的分类能力可能很小，但在随机产生大量的决策树后，一个测试样品可以通过每一棵树的分类结果经统计后选择最可能的分类。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。
<br>在随机森林中，最重要的是如何构造一个随机森林。假设数据样本数为N，那么每棵决策树采样的样本数也就是N，每个样本的属性个数为M，在每个决策树构造过程中，每个节点随机选择m个属性计算最佳分裂方式进行分裂。具体步骤如下:</br> 
* 有放回地随机选择N个样本，用这N个样本来训练一棵决策树;
* 每个样本有M个属性，在决策树中需要分裂节点时，从这M个属性中随机选取m个属性，一般来说m << M，然后从这m个属性中采用某种策略选择最佳属性作为当前节点的分裂属性;
* 每棵决策树的每个节点的分裂都按照步骤（2）迭代进行，直到不能分裂为止;
* 对于每棵决策树都这样建立，就得到了随机森林;
<br>随机森林的随机性体现在每棵树的训练样本是随机的，树中每个节点的分裂属性也是随机选择的。有了这两个随机因素，即使每棵决策树没有进行剪枝，随机森林也不会产生过拟合的现象。随机森林中有两个人为控制参数：森林中树的数量（一般选取值较大）和m值的大小（一般选取为M的平方根）。</br>
<br>随机森林原理是逐步进化而来的，其中ID3树，C4.5树，CART树与之有演进关联，具体实现上Bagging方法和属性随机选择方法结合而来。</br>
##### ID3树
##### C4.5树
##### CART树


#### 提升决策树（Boosted Decision Tress）
#### 决策树构造比较（Comparison Of Three Methods:Bagging,Boosting,and Randomization）
#### 支持向量机(Support Vector Machines)
#### 朴素贝叶斯（NaiveBayes）

### 附录：
